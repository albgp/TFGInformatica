\chapter{Complejidad cuántica.}\label{Comp1}


\section{Los modelos de computación de Turing}

\begin{defin}[Máquina de Turing]\index{Máquina de Turing clásica determinista}
Una \textbf{Máquina de Turing clásica determinista} (abreviado TM) es una hepta-tupla ordenada $(\Sigma, \lambda, Q, q_i, A, F, \delta)$ donde,

\begin{itemize}
\item $\Sigma$ es un conjunto finito, llamado el \textbf{alfabeto} de símbolos.
\item $Q$ es un conjunto finito, llamado el conjunto de \textbf{estados}.
\item $\lambda\in \Sigma$ es un símbolo llamado \textbf{símbolo blanco}.
\item $q_i\in Q$ es el \textbf{estado inicial}.
\item $A\subseteq Q$ es el conjunto de \textbf{estados de aceptación}.
\item $F\subseteq Q$ es el conjunto de \textbf{estados finales}.
\item $\delta: \Sigma \times Q \rightarrow \Sigma \times Q \times \{L,R\}$ es la \textbf{función de transición}.
\end{itemize}
\end{defin}

\begin{defin}[Máquina de Turing probabilista]\index{Máquina de Turing probabilista}
Una máquina de Turing probabilista, que denotaremos como PTM, es una versión ligeramente modificada de la TM clásica donde cambiamos los espacios de actuación de la función de transición, provocando así que la función no devuelva un comportamiento unívoco que la máquina deberá realizar sino una distribución de probabilidad sobre los posibles comportamientos. Será la PTM la encargada de elegir aleatoriamente, aunque siguiendo la distribución de probabilidad que defina la función de transición, qué comportamiento realizar en cada paso. Así pues, una PTM corresponderá a una hepta-tupla $(\Sigma, \lambda, Q, q_i, A, F, \delta)$ donde todos los componentes corresponden en nombre y significado con los definidos para la TM excepto la función $\delta$ que se define en el dominio de $Q\times \Sigma \times Q \times \Sigma\times \{L,R\}$ y con imagen en $[0,1]\subseteq \mathbb{R}^+$. Por tanto la probabilidad de, si nos encontramos en un estado $q_i$ habiendo leído el símbolo $\sigma_1$, pasar al estado $q_2$ moviéndonos en la dirección $D\in\{L,R\}$ y escribiendo el símbolo $\sigma_2$ en la cinta es

\begin{equation}
\delta(q_1,\sigma_1,q_2,\delta_2,D)
\end{equation}

Si la probabilidad de que la PTM pare en un estado de $F\cap A$ es $p$, entonces diremos que la PTM \emph{acepta} con probabilidad $p$.

\end{defin}


\section{La hipótesis de Church-Turing}

Durante la segunda mitad del siglo XX se ha escrito numerosa literatura acerca de las máquinas de computación. Una máquina de computación es, en su forma más fundamental, un dispositivo físico cuya evolución dinámica transforma un conjunto de estados de entrada a un conjunto de estados de salida, estados que supondremos etiquetados en algún conjunto contable, por ejemplo $\mathbb{N}$. Si consideramos una máquina de cómputo clásica determinista como, por ejemplo, una máquina de Turing clásica, podemos ver su funcionamiento como una función $f$ que transforma el estado de entrada al estado de salida de forma determinista y unívoca.\\

Podemos entonces definir la ``equivalencia computacional'' de dos máquinas de computación clásicas y deterministas mediante la igualdad de las funciones que implementan bajo un mismo conjunto de etiquetado. El principal problema surge al intentar definir la equivalencia computacional para máquinas de computación que no son deterministas. \\

Conocemos bien un ejemplo de estas máquinas: Los circuitos cuánticos. Una vez ejecutado el circuito con una entrada determinada, la salida (es decir, el resultado tras la medición) no es, en general, siempre el mismo\footnote{A pesar de que el estado del sistema antes de la medición sí sea determinista.}, dado que está sometido a una aleatoriedad. Por tanto la noción de equivalencia necesitará una generalización para este tipo de máquinas.\\

En las máquinas de computación no deterministas, la salida puede verse como una distribución de probabilidad de un estado definido por un observable (en los circuitos cuánticos, este observable será simplemente la medición que proyecta el estado sobre un estado de la base), así que etiquetaremos la salida de la máquina no determinista como un conjunto ordenado de pares $(O, r)$ donde $r$ es el resultado de la máquina al ser observada con el observable $O$. Tal dispositivo, dada una entrada, definirá una distribución de probabilidad sobre el conjunto de valores de salida. Consideraremos pues que dos máquinas de computación no deterministas serán computacionalmente equivalentes si existe una equivalencia entre los valores de salida de ambas máquinas de forma que una misma entrada define iguales distribuciones de probabilidad sobre los valores de salida relacionados.\\

Tal y como hemos visto, cada máquina de computación $\mathcal{M}$ computa una sola función $f$, aun así, no habría ningún problema en modificar el sistema de cómputo de $\mathcal{M}$ para obtener otra máquina $\mathcal{M}'$ que compute una función diferente. Para formalizar este cambio podemos considerar estas máquinas que computan una sola función como casos particulares de una máquina general $\mathcal{M}(\mathcal{P})$ que actúa sobre la entrada siguiendo las instrucciones codificadas en $\mathcal{P}$, a las que a veces nos referiremos como <<programa>>. Podemos definir entonces el conjunto $C(\mathcal{M})$ como el conjunto de funciones que $\mathcal{M}$ puede computar si se le suministra el programa $\mathcal{P}$ adecuado. Es fácil comprobar que es posible construir, dadas dos máquinas generales $\mathcal{M}$ y $\mathcal{M}'$ una máquina compuesta que compute $C(\mathcal{M})\cup C(\mathcal{M}')$.\\

Así pues, ¿por qué no proceder \textit{ad infinitum} y construir una máquina universal $\tilde{\mathcal{M}}$ que sea capaz de computar cualquier función posible? La realidad es que, físicamente, parece haber un momento en el que añadir más hardware, es decir, crear máquinas de computación más complejas, no permite computar nuevas funciones. De hecho, se puede demostrar que el cualquier para funciones etiquetadas en los números enteros $\mathbb{Z}$, $C(\mathcal{M})$ siempre está contenido en $C(\mathcal{T})$, donde $\mathcal{T}$ es la conocida como máquina de computación universal de Turing \cite{turing1937computable}, es decir, que cualquier $f:\mathbb{Z}\rightarrow\mathbb{Z}$ que sea computable por una máquina de computación universal $\mathcal{M}$ es computable por la máquina universal de Turing $\mathcal{T}$.\\

Este hecho llevó a Alonzo Church \cite{church1936unsolvable} y Alan Turing \cite{turing1937computable} independientemente a conjeturar la llamada hipótesis de Church-Turing, que, en palabras del propio Turing puede formularse como

\begin{hipot}[Hipótesis de Church-Turing] Cada función que puede ser vista de forma natural como ``computable'' puede computarse por una máquina de computación universal de Turing. 
\end{hipot}\index{Hipótesis de Church-Turing}

Esta afirmación, aunque comprensible, no está expresada de un modo formal matemáticamente aceptable. De hecho hay multitud de interpretaciones diferentes, como la expresada en el fantástico libro \cite{hofstadter1980godel} en el que se establece un interesante paralelismo entre las ``funciones que pueden ser vistas de forma natural como computables'' y los cálculos que puede realizar la mente humana.\\

El físico David Deutsch dio una reformulación física no ambigua en el artículo \cite{1985RSPSA.400...97D}, definiendo así el principio de Church-Turing como:

\begin{ppio}[Principio de Church-Turing]\label{CTH} Cada sistema físico finitamente realizable puede ser simulado perfectamente por una máquina de computación de forma finita.
\end{ppio}\index{Principio de Church-Turing}

Donde se define la ``simulación perfecta'' de un proceso físico de un sistema físico $\mathcal{S}$ por una máquina de computación $\mathcal{M}$ si existe un programa $\mathcal{P}(\mathcal{S})$ tal que $\mathcal{M}$ es computacionalmente equivalente a $\mathcal{S}$ bajo una elección apropiada para el etiquetado de sus entradas y salidas. El hecho de que la simulación sea ``de forma finita'' se puede formalizar como sigue. Si consideramos una máquina de computación como una secuencia de pasos cuya duración es estrictamente positiva acotada inferiormente por un valor $\varepsilon$, entonces diremos que la simulación es de forma finita si $(i)$ solo un subsistema finito está en movimiento durante un paso, $(ii)$ el movimiento solamente depende del estado de un subsistema finito, y $(iii)$ la regla que especifica el movimiento puede especificarse formalmente con un número finito de instrucciones. La máquina universal de Turing $\mathcal{T}$ cumple trivialmente estas tres condiciones, así como el computador cuántico universal\footnote{También conocido como máquina de Turing cuántica.} $\mathcal{Q}$ que veremos en la sección siguiente. \\

Así pues, la formulación como principio de la hipótesis de Church-Turing debería incluir cualquier sistema físico que fuese realizable experimentalmente y la máquina de computación debería ser finitamente especificable.\\

El problema que surge al pensar más profundamente en el principio \ref{CTH} es que un sistema físico general no es simulable por una máquina de Turing universal, dado que sus estados forman un continuo\footnote{Imaginemos como ejemplo un sistema físico simple, un péndulo. No es difícil comprobar que el péndulo puede tomar un número continuo (y por tanto infinito) de valores para la posición} y la máquina universal de Turing tan solo puede trabajar con valores en un conjunto numerable. Sin embargo no debemos darnos por vencidos, pues se puede demostrar que el computador cuántico universal $\mathcal{Q}$ puede simular cualquier proceso real (es decir, disipativo). Así pues, la teoría cuántica es perfectamente compatible con el principio de Church-Turing en su versión física, no así la computación clásica.

\section{El computador cuántico universal.}\index{Máquina de Turing cuántica}

Una máquina de Turing cuántica (o, abreviadamente, \emph{QTM}) es similar a una máquina de Turing probabilista a excepción de las siguientes diferencias:

\begin{enumerate}
\item Los coeficientes no son probabilidades sino números complejos que llamaremos <<amplitudes>>.

\item En cada paso, los cuadrados de los módulos de las amplitudes suman $1$.

\item Para cualquier entrada, la matriz de transición debe ser unitaria.
\end{enumerate}

Establecemos la condición de parada de una QTM como que cada una de sus bifurcaciones alcance un estado final. En tal caso la salida estará escrita en la cinta desde la posición inicial hasta el primer símbolo blanco. La probabilidad de que la salida sea una determinada configuración viene dada por el módulo al cuadrado de la amplitud correspondiente.\\


\begin{redBox}
\begin{defin}[Máquina de Turing cuántica]
Una \textbf{Máquina de Turing cuántica} es una hexa-tupla ordenada $(\Sigma, \lambda, Q, q_i, q_f, \delta)$ donde,

\begin{itemize}
\item $\Sigma$ es un conjunto finito, llamado el \textbf{alfabeto} de símbolos. Asumimos que $\Sigma=\{0,1,\lambda\}$
\item $Q$ es un conjunto finito, llamado el conjunto de \textbf{estados}.
\item $\lambda\in \Sigma$ es un símbolo llamado \textbf{símbolo blanco}.
\item $q_i\in Q$ es el \textbf{estado inicial}.
\item $A\subseteq Q$ es el conjunto de \textbf{estados de aceptación}.
\item $q_f$ es el \textbf{estado final}.
\item La \textbf{función de transición} es $\delta: \Sigma \times Q \rightarrow H $ donde $H$ es el espacio de Hilbert complejo generado por los vectores correspondientes a las ternas de $\Sigma \times Q \times \{L,R\}$.
\end{itemize}

y además, la matriz de transición es unitaria para cualquier entrada.
\end{defin}
\end{redBox}

\begin{defin}[Simulación]
Decimos que una QTM $Q$ simula un circuito cuántico $C$ para una entrada $I$ si $Q$, proporcionada la entrada $I$, resulta una distribución de probabilidad idéntica a la que proporciona $C$.
\end{defin} \index{Simulación circuital}

\section{Máquinas de Turing cuánticas y circuitos cuánticos.}

\begin{lema} Cada matriz de tamaño $2^k\times 2^k$ puede descomponerse en, como máximo, $2^{O(k)}$ puertas cuánticas de un solo qubit y puertas CNOT.
\end{lema}

\begin{proof}
Ver \cite{1995PhRvA..52.3457B}
\end{proof}

\begin{lema} Para cada matriz unitaria $U$ de tamaño $2^k\times 2^k$ existe una QTM que simula el circuito consistente en tan solo la puerta $U$.
\end{lema}

\begin{proof}
Como una matriz unitaria $U$ se puede ver como una función $f_U:H^{\otimes k}\rightarrow H^{\otimes k}$ definida en el espacio de Hilbert de estados de un registro de $k$ qubits, tal función puede implementarse con una QTM.
\end{proof}

\begin{redBox}
\begin{prop}
Para cada circuito cuántico de tamaño $n$ existe una máquina de Turing cuántica con complejidad $T(n)$ que simula tal circuito tal que $T(n)=O(n)$.
\end{prop}
\end{redBox}

\begin{proof}
Cada puerta cuántica de tamaño $2^k$ puede simularse en una QTM usando, como máximo, $2^{O(k)}$ pasos, así pues, si acotamos el tamaño máximo de las puertas cuánticas que usamos (es decir, de las puertas del conjunto universal que consideremos) como hicimos en \ref{},  a $2^m$, la simulación necesitará como máximo $2^{O(m)}n=O(n)$ pasos.\\
\end{proof}

\begin{redBox}
\begin{prop}\label{unaMas}
Para cada entero positivo $n$ y para cada QTM $M$ con complejidad $T$ existe un circuito con $poly(n,T)$ puertas cuánticas elementales que simula $M$ para cualquier entrada de tamaño $n$.
\end{prop}
\end{redBox}

\begin{proof}
Para cada uno de los $T$ pasos que realiza $M$ construiremos un circuito diferente. Como la QTM no puede recorrer más de $T$ celdas desde la posición inicial supondremos que la cinta es finita con tan solo $2T+1$ celdas. Así pues, para cada una de estas celdas añadimos $l=1+\lceil\log(|Q|+1) \rceil)+\lceil\log|\Sigma | \rceil$ conexiones al circuito, donde cada una de ellas se usa para lo siguiente:

\begin{itemize}
\item Usamos las $\lceil\log(|Q|+1) \rceil)$ conexiones para codificar el estado actual, donde suponemos que puede existir un nuevo estado (por ello se suma la constante $1$) que codificará que la máquina nunca ha alcanzado esa celda.

\item Usamos los $\lceil\log|\Sigma | \rceil$ bits para codificar el símbolo escrito en esa celda.

\item Por último utilizamos una conexión más para indicar si la cabeza lectora-escritora de la QTM está en esa celda concreta.
\end{itemize}

Así pues, para cada paso de la ejecución de la QTM, centrémonos en la celda sobre la que la cabeza está situada. Queremos definir una matriz unitaria que transforme los estados de las $3l$ conexiones según como lo haría la función de transición $\delta$. Lo escribimos formalmente como

\begin{equation}
\begin{split}
U\Big(\ket{n,a_l,0}\ket{q_1,a_1,1}\ket{n,a_r,0}\Big)&=\sum_{a',q'}\delta(q,a,q',a',L)\ket{q',a_l,1}\ket{n,a',0}\ket{n,a_r,0}\\
&+\delta(q,a,q',a',R)\ket{n,a_l,0}\ket{n,a',0}\ket{q',a_r,1}
\end{split}
\end{equation}

Pero podemos demostrar que los vectores  $U\Big(\ket{n,a_l,0}\ket{q_1,a_1,1}\ket{n,a_r,0}\Big)$ son mutuamente ortogonales entre sí:

\begin{equation}
\begin{split}
\bra{s,a_{l1},0}\bra{q_1,a_1,1}\bra{s,a_{r1},0}U^\dag U\ket{s,a_{l2},0}\ket{q_2,a_2,1}\ket{s,a_{r2},0}=\\
\Big(\sum_{a_1',q_1'}\Big\{\delta(q_1,a_1,q_1',a_1',L)\bra{q_1'a_{l1},1}\bra{s,a_1',0}\bra{s,a_{r1},0}+\\
\delta(q_1,a_1,q_1',a_1',R)\bra{s,a_{l1},0}\bra{s,a_1',0}\bra{q_1',a_{r1},1}\Big\}\Big)\\
\Big(\sum_{a_2',q_2'}\Big\{\delta(q_2,a_2,q_2',a_2',L)\bra{q_2'a_{l2},1}\bra{s,a_2',0}\bra{s,a_{r2},0}+\\
\delta(q_2,a_2,q_2',a_2',R)\bra{s,a_{l2},0}\bra{s,a_2',0}\bra{q_2',a_{r2},1}\Big\}\Big)=\\
\sum_{a_q',q_1',a_2',q_2'}\delta(q_1,a_1,q_1',a_1',L)\delta(q_2,a_2,q_2',a_2',L)\Big(\\
\bra{q_1'a_{l1},1}\bra{s,a_1',0}\bra{s,a_{r1},0}\ket{s,a_{r2},0}\ket{s,a_2',0}\ket{q_2'a_{l2},1}\Big)+\\
\delta(q_1,a_1,q_1',a_1',R)\delta(q_2,a_2,q_2',a_2',R)\Big(\\
\bra{s,a_{l1},0}\bra{s,a_1',0}\bra{q_1',a_{r1},1}\ket{q_2',a_{r2},1}\ket{s,a_2',0}\ket{s,a_{l2},0}\Big)=\\
\delta^{a_{l1}}_{a_{l2}}\delta^{a_{r1}}_{a_{r2}}\Big(\sum_{a_1',q_1'}\delta(q_1,a_1,q_1',a_1',L)\delta(q_2,a_2,q_2',a_2',L)+\delta(q_1,a_1,q_1',a_1',R)\delta(q_2,a_2,q_2',a_2',R)\Big)\\
=0
\end{split}
\end{equation}


Donde $\delta_m^n$ es la función delta de Kronecker y donde la última igualdad surge de la condición de las QTM de tener transiciones unitarias, ya que todas las filas distintas de la matriz de transición serán ortogonales. El resto de los vectores de la base pueden establecerse entonces de forma que $U$ sea una matriz ortogonal. Si añadimos al circuito una de estas puertas a cada terna de celdas adyacentes (no importa el orden porqueee...) este circuito simulará un paso de $M$. Para simular los $T$ pasos necesitaremos $T$ de estos circuitos.\\

Hemos usado $O(T\cdot (2T+1))$ matrices de tamaño $2^{3l}\times 2^{3l}$ y $2O(l(2T+1))$ conexiones. Por el teorema \ref{teoUniv} sabemos que tales matrices pueden realizarse con $2^{O(l)}$ puertas cuánticas elementales, por tanto hemos usado $T^22^{O(l)}$ puertas cuánticas elementales.\\

\end{proof}

\section{Definiciones de la clase \texorpdfstring{$\BQP$}{BQP}.}

\begin{redBox}
\begin{defin}[$\BQP$ mediante QTM.] \index{Clase $\BQP$}
Un lenguaje $\mathcal{L}$ está contenido en $\BQP$ si existe un polinomio $p(n)$ tal que $\mathcal{L}$ es aceptado por una máquina de Turing cuántica con complejidad temporal $p(n)$.
\end{defin}
\end{redBox}

Gracias a la proposición \ref{unaMas} podemos definir la clase $\BQP$ usando circuitos cuánticos como sigue\\

\begin{redBox}
\begin{defin}[$\BQP$ mediante circuitos.]
Un lenguaje $\mathcal{L}$ está contenido en $\BQP$ si existe una función $f(n)$ y polinomios $p(n)$, $q(n)$ tal que para cada $n$ la salida de $f(n)$ es un circuito C de anchura $n$ y tamaño $p(n)$ tal que C acepta el lenguaje $\mathcal{L}_n\equiv \{x\in\mathcal{L}\ |\  |x|=n\}$ y el tiempo de ejecución de $f(n)$ es, como máximo, $q(n)$.
\end{defin}
\end{redBox}

\section{Caracterización de \texorpdfstring{$\BQP$}{BQP}}

Un resultado importante que nos permitirá relacionar la complejidad cuántica con la clásica es el hecho de que cada circuito clásico (booleano) puede implementarse eficientemente dentro de un circuito cuántico. De hecho, demostraremos que el número de puertas necesarias en un circuito cuántico tiene exactamente el mismo orden que el número de puertas booleanas del circuito que implementa.\\

\begin{teo}\label{teoEquivCirc} Si $f:\{0,1\}^n\rightarrow\{0,1\}^m$ es computable por un circuito booleano de tamaño $S$, entonces existe una secuencia de $2S+m+n$ puertas cuánticas que computan la operación \footnote{De dónde sale el $n$!!}
\begin{equation}
\ket{x}\ket{0^{2m+S}} \rightarrow \ket{x}\ket{f(x)}\ket{0^{S+m}}
\end{equation}
\end{teo}

\begin{proof}
El registro que utilizaremos para el circuito cuántico será un registro compuesto por $n+2m+S$ qubits, en la sección correspondiente a los $n$ primeros qubits almacenaremos el valor de entrada sobre el que queremos computar $f$. Los $2m$ qubits a continuación los usaremos para almacenar el valor de la función ya computada y una copia\footnotemark{} de ella. Por último los $S$ qubits restantes son los conocidos como \emph{scratchpad}, es decir, unos qubits necesarios para mantener la reversibilidad del circuito.\\

\footnotetext{Blah blah \ref{NC}.}

La primera parte del circuito corresponderá al circuito booleano en el que reemplazamos cada puerta lógica clásica (AND, OR, NOT) por su análogo cuántico visto en la sección \ref{}, y en el que la entrada no son tan solos los $n$ qubits que corresponderían a la entrada de la función sino los $n+2m+S$ qubits que necesitaremos para el circuito.\\

Si la entrada al circuito es $\ket{x}\ket{0^{2m+S}}$ el resultado del cómputo será $\ket{x}\ket{f(x)0^m}\ket{z}$, que podremos realizar usando tan solo $S$ puertas cuánticas. Tras este cómputo copiamos el valor $f(x)$ a los $m$ registros siguientes, aún con el valor $0$, usando $m$ operaciones de la forma $\ket{bc}\rightarrow\ket{b(b\oplus c)}$. Si aplicamos en este punto una a una las inversas de las $S$ puertas cuánticas en sentido contrario, esta operación eliminará el registro $f(x)$ original así como los valores $\ket{z}$ del scratchpad, dejándolos en el valor $\ket{0}$ de nuevo, alcanzando así el estado del enunciado.
\end{proof}

Para los teoremas que siguen recomendamos revisar las definiciones de las clases de complejidad clásica de un texto como \cite{papadimitriou2003computational} o \cite{sipser2006introduction}.\\

\begin{corol} $\PP\subseteq \BQP$
\end{corol}


\begin{proof}

Sabemos que cada TM clásica $M$ con orden de complejidad $T(n)$ tiene un circuito booleano equivalente con $O(T(n)\log T(n))$ puertas lógicas. Por el teorema anterior existirá por tanto un circuito cuántico con $O(T(n)\log T(n) + n)$ puertas cuánticas que computará la misma función que $M$ lo que, usando la caracterización de $\BQP$ mediante circuitos cuánticos, prueba la afirmación.
\end{proof}

\begin{corol} $\BPP\subseteq \BQP$
\end{corol}

\begin{proof}
Ver \cite{arora2009computational}
\end{proof}

Podemos comprobar que al menos, la computación cuántica no es infinitamente más potente que la clásica mediante los siguientes dos resultados:

\begin{teo} $\BQP \subseteq \PSPACE$
\end{teo}

\begin{proof}
Ver \cite{arora2009computational}
\end{proof}

\begin{corol}
$\BQP \subseteq \mathbf{EXPTIME}$
\end{corol}
\begin{proof}
A pesar de que se sigue directamente de que $\PSPACE \subseteq \mathbf{EXPTIME}$, se puede consultar \cite{book:929225} para una demostración explícita.
\end{proof}


Sin embargo, la veracidad de las siguientes afirmaciones sigue todavía siendo un problema abierto en teoría de la computación
\begin{enumerate}
\item $\PP\myEq{?}\BQP$
\item $\BPP\myEq{?}\BQP$
\item $\NP\myEq{?}\BQP$
\item $\PH\myEq{?}\BQP$
\end{enumerate}

A pesar de que no exista una demostración todavía para las afirmaciones mencionadas, los teóricos de la computación creen tener una intuición sobre aquellas que serían falsas. La segunda de ellas, $\BPP\myEq{?}\BQP$, se cree falsa (\cite{book:929225}) por la razón de que no se ha encontrado ningún algoritmo probabilista capaz de factorizar en tiempo polinomial sobre el tamaño de la entrada. La primera sería, usando la conocida relación de inclusión $\PP\subseteq\BPP$, también falsa. Por otro lado no se cree que $\NP=\BQP$ dado que tal afirmación significaría que la computación cuántica es capaz de resolver cualquier problema $\NP$ en orden temporal lineal. Sin embargo, aún es posible que, contra la intuición de muchos científicos, alguna de estas afirmaciones resulte cierta provocando así profundos cambios sobre nuestra concepción de la potencia de los modelos de computación tanto clásica como cuántica.\\

Por otro lado, a pesar de que tales problemas estén a día de hoy aún sin resolver, se han encontrado oráculos $O$ y $\tilde{O}$ relativos a los cuales $\PP^O=\BQP^O$ (\cite{FORTNOW1999240}) y $\PH^{\tilde{O}}=\BQP^{\tilde{O}}$ (\cite{raz2018oracle}) tales que no colapsan la jerarquía, es decir, $\PP^O\neq \NP^O$ y $\PP^{\tilde{O}}\neq \NP^{\tilde{O}}$.

%\subsection{\texorpdfstring{Relación entre $\PP $ y $ \BQP$.}{P en BQP}}
%\section{\texorpdfstring{La jerarquía de clases}{La jerarquía de clases}}
%
%\begin{redBox}
%\begin{cuest} \textbf{¿}$\BQP \subseteq \NP$\textbf{?}
%\end{cuest}
%\end{redBox}
%

\begin{minipage}{\linewidth}
      \centering
      \begin{minipage}{0.45\linewidth}
          \begin{figure}[H]
			\begin{center}
				\includegraphics[scale=0.5]{../Diagramas/LOffice-crop.pdf}
			\end{center}
			\caption{Posible relación entre $\PP$, $\BQP$, $\NP$ y $\PSPACE$}
		\end{figure}
      \end{minipage}
      \hspace{0.05\linewidth}
      \begin{minipage}{0.45\linewidth}
          \begin{figure}[H]
			\begin{center}
				\includegraphics[scale=0.485]{../Diagramas/Inclusion-crop.pdf}
			\end{center}
			\caption{Jerarquía de inclusión conocida para la clase $\BQP$}
		\end{figure}
      \end{minipage}
  \end{minipage}

%
%\begin{redBox}
%\begin{prop} $\BQP\subseteq \mathbf{PP}$
%\end{prop}
%\end{redBox}
%
%\begin{equation}
%\PP^O=\BQP^O
%\end{equation}
%\section{Separación de \texorpdfstring{$\BQP$}{BQP} y \texorpdfstring{$\PH$}{PH} mediante oráculos.} \label{sec:BQPPH}
%
%\begin{figure}
%\begin{center}
%\includegraphics[scale=0.5]{../Diagramas/SeparOraculo-crop.pdf}
%\end{center}
%\caption{Separación entre $\BQP^O$ y $\PH^O$}
%\end{figure}
%
%\begin{redBox}
%\begin{defin}[Distinguibilidad de distribuciones de probabilidad]
%Sean $\mathcal{D}$ y $\mathcal{D}'$ dos distribuciones de probabilidad sobre un conjunto finito $F$. Diremos que un algoritmo $A$ distingue entre $\mathcal{D}$ y $\mathcal{D}'$ con ventaja $\varepsilon$ si
%
%\begin{equation}
%\varepsilon=\Big| \underset{x\sim \mathcal{D}}{\mathbf{Pr}}[A\text{ acepta }x]-\underset{x' \sim \mathcal{D}'}{\mathbf{Pr}}[A\text{ acepta }x'] \Big|
%\end{equation}
%
%\end{defin}
%\end{redBox}
%
%\subsubsection*{Distribuciones de probabilidad $\mathcal{G}$, $\mathcal{G}'$ y $\mathcal{D}$}
%Sea $n\in\mathbb{N}$ y $N=2^n$, definimos una distribución de probabilidad $\mathcal{G}$ en $\mathbb{R}^N\times \mathbb{R}^N$ tal que elegimos una muestra como sigue:
%
%\begin{itemize}
%\item Tomamos $N$ valores $x_1,\cdots,x_N$ aleatoriamente e independientemente según una distribución $\mathcal{N}(0,1)$.
%\item Calculamos el vector $y=H_N\cdot x$ conde $H_N$ es la función de Hadamard definida en \ref{}.
%\item El valor obtenido será $z=(x,y)$
%\end{itemize}
%
%Por el teorema \ref{}, $\mathcal{G}$ es una función normal multivariable con matriz de covarianza
%\begin{equation}
%\begin{pmatrix}
%I_N & H_N \\
%H_N & I_N
%\end{pmatrix}
%\end{equation}
%
%Además, por la proposición \ref{independencia}, como $x_1,\cdots,x_N$ son independientes, también lo serán $y_1,\cdots,y_N$.\\
%
%Sean $S,T$ subconjuntos de $\{1,\cdots,N\}$, para temas que serán relevantes más adelante en la demostración, nos gustaría analizar los momentos de la distribución $\mathcal{G}$ definidos como
%\begin{equation}
%\alpha_{(S,T)}\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny def}}}{=}}\underset{(x,y)\sim\mathcal{G}}{\mathbf{E}}\Big[\prod_{i\in S}x_i\prod_{j\in T}y_j\Big]
%\end{equation}
%
%\begin{redBox}
%\begin{lema}
%Sean $S,T$ subconjuntos de $\{1,\cdots,N\}$, y además $i,\in S,\ j\in T$. Si denotamos $k_1=|S|$ y $k_2=|T|$ entonces
%\begin{itemize}
%\item $\alpha_{(\{i\},\{j\})}=N^{-1/2}\cdot(-1)^{<i,j>}$.
%\item $\alpha_{(S,T)}=0 \ \text{si } k_1\neq k_2$
%\item $|\alpha_{(S,T)}|\leq k!\cdot N^{-k/2}\ \text{si }k=k_1=k_2$
%\item $|\alpha_{(S,T)}|\leq 1\ \forall S,T$
%\end{itemize}
%\end{lema}
%\end{redBox}
%\begin{proof}
%\todo{Revisar. Son covarianzas!}
%$i)$ Este momento es simplemente un elemento de la matriz de covarianzas perteneciente al segundo o tercer cuadrante, es decir, a la parte correspondiente a la función de Hadamard, que toma esa forma por el lema \ref{}.\\
%
%$ii)$ Usando el teorema \ref{Isserlis} podemos escribir
%\begin{equation}
%\alpha_{(S,T)}=\sum\prod\mathbf{E}[Z_{i_r}Z_{i_l}]
%\end{equation}
%Donde los pares $(i_r,i_l)$ cubren todos los enteros pertenecientes a $S$ y $T$ tan solo una vez en cada sumando. Dado que $|S|\neq|T|$ (y suponemos sin pérdida de generalidad que $|S|>|T|$ ) alguno de esos pares, llamémosle $(i_a,i_b)$ debe de estar contenido en $S$, por lo que el término $\mathbf{E}[Z_{i_a}Z_{i_b}]$ será cero (pues es el producto de normales idénticas con media $0$), por lo que en concreto todo el producto será $0$ y por tanto, el sumatorio y el propio $\alpha_{(S,T)}$ serán $0$.\\
%
%$iii)$ Dado que cada sumando corresponde a una partición de los enteros en $S$ y $T$, existen ${2k}\choose{2}$ sumandos pues hay ${2k}\choose{2}$ formas de particionar los conjuntos en pares, pero podemos ver que tan solo existen $k!$ formas de elegir los pares de forma que cada uno de los pares $(i_a,i_b)$ pertenezcan a un conjunto distinto (a $S$ o a $T$) y el valor para estos pares, por el apartado $i)$ es $\pm N^{-1/2}$. Sumando el $k$-producto de estos valores (positivos siempre pues tomamos valores absolutos) para las ya comentadas $k!$ posibles elecciones se obtiene la afirmación.\\
%
%$iv)$ Se obtiene directamente de la famosa desigualdad de Cauchy–Bunyakovsky–Schwarz:
%\begin{equation}
% \alpha_{(S,N)}=\underset{(x,y)\sim\mathcal{G}}{\mathbf{E}}\Big[\prod_{i\in S}x_i\prod_{j\in T}y_j\Big]\leq
% \sqrt{\mathbf{E}\Big[\prod_{i\in S}x_i^2\Big]\mathbf{E}\Big[\prod_{j\in T}y_i^2\Big]}=
% \sqrt{\prod_{i\in S}\mathbf{E}\Big[x_i^2\Big]\prod_{j\in T}\mathbf{E}\Big[y_i^2\Big]}=1
%\end{equation}
%\end{proof}
%
%En este punto ya hemos estudiado las propiedades de la distribución $\mathcal{G}$. A partir de ella definiremos la distribución $\mathcal{G}'$.\\
%
%Sea $\varepsilon=\frac{1}{24\log N}$, definimos la distribución $\mathcal{G}'$ sobre $\mathbb{R}^2N$ de forma que devuelve los valores de $\mathcal{G}$ multiplicados por $\sqrt{\varepsilon}$. Bajo estas hipótesis, $\mathcal{G}'$ es una normal multivariable con matriz de covarianza
%
%\begin{equation}
%\varepsilon\begin{pmatrix}
%I_N & H_N \\
%H_N & I_N
%\end{pmatrix}
%\end{equation}
%
%Por último, partiendo de la distribución modificada $\mathcal{G}'$, definimos la distribución $\mathcal{D}$ como sigue:
%\begin{itemize}
%\item tomamos un valor $z\sim \mathcal{G}'$
%\item truncamos sus valores, $\mathbf{truncate}(z)=(\mathbf{truncate}(z_1),\cdots,\mathbf{truncate}(z_{2N}))$
%\item Independientemente para cada índice, tomamos el valor $z'_i=1$ con probabilidad $\frac{1+\mathbf{truncate}(z_i)}{2}$ y el valor $z'_i=1$ con probabilidad $\frac{1-\mathbf{truncate}(z_i)}{2}$.
%\item Devolvemos $z'$.
%\end{itemize}
%
%\begin{figure}
%\centering
%\begin{subfigure}{.5\textwidth}
%  \centering
%  \includegraphics[width=.8\linewidth]{../Diagramas/DistD18-crop.pdf}
%  \caption{}
%\end{subfigure}%
%\begin{subfigure}{.5\textwidth}
%  \centering
%  \includegraphics[width=.8\linewidth]{../Diagramas/DistD14-crop.pdf}
%  \caption{}
%\end{subfigure}
%\caption{(a) Gráfica con la función de densidad de $x_i\sim\mathcal{N}(0,\varepsilon)$ y el histograma para $N=2^{18}$ y (b) Gráfica para la distribución $\mathcal{G}'$ con $N=2^{14}$ recortada en el eje $y$ para mejor visualización.}
%\end{figure}
%
%\subsubsection*{Funciones multilineales en $\mathcal{D}$}
%
%En este punto nos gustaría probar que la distribución $\mathcal{D}$ que toma los valores discretos $\{-1,1\}$ y la distribución $\mathcal{G}'$ que toma valores continuos $[-1,1]$ tienen la misma esperanza. De hecho, generalizaremos este hecho y probaremos que cualquier para cualquier funcion multilineal $F:\mathbb{R}^{2N}\rightarrow\mathbb{R}$ entonces la esperanza de $F(\mathcal{D})$ y la de $F(\mathcal{G}')$ coinciden, es decir,
%
%\begin{equation} \label{eq1:multilineal}
%\underset{z'\sim\mathcal{D}}{\mathbf{E}}[F(z')]=\underset{z\sim\mathcal{G}'}{\mathbf{E}}[F(z)]
%\end{equation}
%
%Probaremos primero algo que puede parecer ligeramente intuitivo. Si consideramos la distribución $\mathcal{G}''$ consistente en truncar los valores fuera del rango $[-1,1]$ de la distribución $\mathcal{G}'$ tal y como se ha realizado en el primer paso de la definición de la distribución $\mathcal{D}$, queremos probar la versión <<preliminar>> de la ecuación \ref{eq1:multilineal}, que será que la esperanza de $F$ mediante $\mathcal{D}$ y mediante $\mathcal{G}''$ coincide, es decir
%
%\begin{equation} \label{eq2:multilineal}
%\underset{z'\sim\mathcal{D}}{\mathbf{E}}[F(z')]=\underset{z\sim\mathcal{G}''}{\mathbf{E}}[F(z)]
%\end{equation}
%
%Pero esto es fácilmente visible siguiendo el siguiente argumento: \\
%
%(...)\\
%
%Por tanto, habiendo probado la ecuación \ref{eq2:multilineal}, intentemos acotar la diferencia entre las esperanzas $\underset{z\sim\mathcal{G}''}{\mathbf{E}}[F(z)]$ y $\underset{z\sim\mathcal{G}'}{\mathbf{E}}[F(z)]$. Sin embargo, vemos que si el vector $z$ obtenido de $\mathcal{G}'$ tiene todos sus valores en el rango $[-1,1]$ entonces el efecto del truncado es nulo y por tanto, para todos esos valores, $\mathcal{G}'$ coincide con $\mathcal{G}''$. Veamos qué ocurre si $z$ no pertenece al rango $[-1,1]^{2N}$.\\
%
%El hecho de que la función $F$ sea multilineal nos permite acotar su valor absoluto $F$ para los valores fuera de rango, cuya cota vemos en el siguiente lema, que probaremos en el anexo, por no ser crucial para el tema que nos trata.
%
%\begin{lema}  \label{lem1:multilineal} Sea $F:\mathbb{R}^{2N} \rightarrow \mathbb{R}$ una función multilineal que lleva los valores $\{\pm 1\}^{2N}$ a $[-1,1]$ y sea $z\in\mathbb{R}^{2N}$ entonces
%\begin{equation}
%|F(x)|\leq \prod_{i=1}^{2N} \max(1,|z_i|)
%\end{equation}
%\end{lema}
%
%\begin{lema}\label{lem2:multilineal}
%\begin{equation}
%\underset{(x,y)\sim\mathcal{G}'}{\mathbf{E}}\Big[ \prod_{i=1}^N\max(1,|x_i|)\cdot\prod_{i=1}^N\max(1,|y_i|)\cdot \mathlarger{\mathlarger{\mathlarger{\chi}}}_{\{(x,y)\neq(\mathbf{truncate}(x,y))\}} \Big]
%\end{equation}
%\end{lema}
%
%\begin{lema} \label{lem3:multilineal} Sean $0\leq p,\ p_0$ tales que $p+p_0\leq 1$. Si $F:\mathbb{R}^{2N} \rightarrow \mathbb{R}$ es una función multilineal que lleva los valores $\{\pm 1\}^{2N}$ a $[-1,1]$. Si tomamos $z_0\in[-p_0,p_0]^{2N}$ entonces
%\begin{equation}
%\underset{z\sim\mathcal{G}'}{\mathbf{E}}[|F(\mathbf{truncate}(z_0+p\cdot z))-F(z_0+p\cdot z)|]\leq \frac{8}{N^2}
%\end{equation}
%\end{lema}
%
%
%
%\subsubsection*{Algoritmo cuántico para distinguir entre $\mathcal{D}$ y la distribución uniforme $U_{2N}$.}
%
%\begin{redBox}
%\begin{lema}
%Existe un algoritmo cuántico $Q$ que, dada una entrada $(x,y)\in\{\pm 1\}^{N}\times \{\pm 1\}^{N}$, $Q$ la acepta con probabilidad $\frac{1+\varphi(x,y)}{2}$ donde
%\begin{equation}
%\varphi(x,y):=\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^N x_i\cdot H_{i,j}\cdot y_j
%\end{equation}
%\end{lema}
%\end{redBox}
%
%
%\begin{lema}
%$\underset{(x,y)\sim U_{2N}}{\mathbf{E}}[\varphi(x,y)]=0$
%\end{lema}
%
%\begin{proof}
%Para cada $i,j=1,\cdots,N$ tenemos que $\underset{(x,y)\sim U_{2N}}{\mathbf{E}}[x_iy_i]=0$ dado que la distribución uniforme tiene media cero. Por lo que, usando la linealidad de la esperanza matemática,
%
%\begin{equation}
%\begin{split}
%\underset{(x,y)\sim U_{2N}}{\mathbf{E}}[\varphi(x,y)]&=\underset{(x,y)\sim U_{2N}}{\mathbf{E}}[\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^N x_i\cdot H_{i,j}\cdot y_j]\\
%&=\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^N H_{i,j}\cdot\underset{(x_i,y_j)\sim U_{2}}{\mathbf{E}} [x_i\cdot y_j]
%\end{split}
%\end{equation}
%\end{proof}
%
%\begin{lema}
%$\underset{(x,y)\sim \mathcal{G}'}{\mathbf{E}}[\varphi(x,y)]=\varepsilon$
%\end{lema}
%
%\begin{proof}
%Usando que $\underset{(x,y)\sim \mathcal{G}'}{\mathbf{E}}[x_iy_j]=\varepsilon\cdot H_{i,j}$ ¿por qué? y usando la linealidad de la esperanza, podemos escribir
%\begin{equation}
%\begin{split}
%\underset{(x,y)\sim \mathcal{G}'}{\mathbf{E}}[\varphi(x,y)]&=\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^N H_{i,j}\cdot\underset{(x,y)\sim \mathcal{G}'}{\mathbf{E}} [x_i\cdot y_j]\\
%&=\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^N H_{i,j}^2 \varepsilon=\frac{N}{N}\varepsilon=\varepsilon
%\end{split}
%\end{equation}
%Ya que, como vimos en \ref{}, $H_N^2=Id_N$, por lo que $H_{i,j}^2$ será $1$ si $i=j$ y $0$ en otro caso.
%\end{proof}
%
%\begin{lema}
%$\underset{(x',y')\sim \mathcal{D}}{\mathbf{E}}[\varphi(x',y')]\geq \frac{\varepsilon}{2}$
%\end{lema}
%
%\begin{proof}
%Como $\varphi$ es multilineal, usando la ecuación \ref{eq2:multilineal} que vimos en la sección anterior, tenemos que
%\begin{equation}
%\begin{split}
%\underset{(x',y')\sim \mathcal{D}}{\mathbf{E}}[\varphi(x',y')]&=\underset{(x,y)\sim \mathcal{G}'}{\mathbf{E}}[\varphi(\mathbf{truncate}(x),\mathbf{truncate}(y))]\\
%&\geq \underset{(x,y)\sim \mathcal{G}'}{\mathbf{E}}[\varphi(x,y)]-\Big|\underset{(x,y)\sim \mathcal{G}'}{\mathbf{E}}[\varphi(\mathbf{truncate}(x),\mathbf{truncate}(y))-\varphi(x,y)]\Big|\\
%&\geq \varepsilon -\underset{(x,y)\sim \mathcal{G}'}{\mathbf{E}}[|\varphi(\mathbf{truncate}(x),\mathbf{truncate}(y))-\varphi(x,y)|]
%\end{split}
%\end{equation}
%
%Basta entonces demostrar que $\underset{(x,y)\sim \mathcal{G}'}{\mathbf{E}}[|\varphi(\mathbf{truncate}(x),\mathbf{truncate}(y))-\varphi(x,y)|]$ está acotada superiormente por $\frac{\varepsilon}{2}$.\\
%
%Como $\varphi(x,y)\in[-1,1]$ para cualquier entrada $(x,y)\in{\pm 1}^{2N}$ y además es multilineal podemos aplicar el lema \ref{lem3:multilineal} con $p_0=0$, $p_1=1$, $z_0=(\underbrace{0,0,\cdots,0}_{2N})$ obteniendo
%
%\begin{equation}
%\underset{(x,y)\sim \mathcal{G}'}{\mathbf{E}}[|\varphi(\mathbf{truncate}(x),\mathbf{truncate}(y))-\varphi(x,y)|]\leq \frac{8}{N^2}=\frac{\varepsilon}{2}
%\end{equation}
%
%\end{proof}
%
%\begin{redBox}
%\begin{prop} Existe un algoritmo cuántico $Q$ que realiza una sola query y que se ejecuta en tiempo $O(\log N)$ tal que
%\begin{equation}
%|\underset{z'\sim \mathcal{D}}{\mathbf{E}}[Q(z')]-\underset{u\sim U_{2N}}{\mathbf{E}}[Q(u)]|\geq \frac{\varepsilon}{2}
%\end{equation}
%\end{prop}
%\end{redBox}
%
%\subsubsection*{$D$ no es distinguible por circuitos booleanos acotados en profundidad.}
%
%Sea $A:\{\pm 1\}^{2N}\rightarrow\{\pm 1\}$ una función booleana, podemos considerar su extensión multilineal a todo $\mathbb{R}^{2N}$ definiendo  $A:\mathbb{R}^{2N}\rightarrow\mathbb{R}$ en $x\in\mathbb{R}^{2N}$ como
%
%\begin{equation}
%A(x)=\sum_{S\subseteq \{1,\cdots, 2N\}}\hat{A}(S)\prod_{i\in S}x_i
%\end{equation}
%
%Donde además podemos ver que $A(0)=\hat{A}(\emptyset)=\underset{x\sim U_{2N}}{\mathbf{E}}[A(x)]$
%
%\begin{lema} Sea $p\leq 1/2$ y $A:\{\pm 1\}^{2N}\rightarrow\{\pm 1\}$ un circuito booleano de tamaño acotado superiormente por $s$ y acotado superiormente en profundidad por $d$ tal que
%\begin{equation}
%\sqrt{\varepsilon}p\cdot (c\log s)^{d-1}\leq 1/2
%\end{equation}
%
%y sea $P\in[-p,p]^{2N}$. Entonces
%\begin{equation}
%\Big|\underset{z\sim \mathcal{G}'}{\mathbf{E}}[A(P \circ z)]-A(0) \Big|\leq 3\epsilon \cdot p^2\cdot(s\log s)^{2(d-1)}N^{-1/2}
%\end{equation}
%\end{lema}
%
%\begin{proof}
%Como $A(0)=\hat{A}(\emptyset)$, entonces
%\begin{equation}
%\begin{split}
%\Big|\underset{z\sim \mathcal{G}'}{\mathbf{E}}[A(P \circ z)]-A(0) \Big|&=\Big|\underset{z\sim \mathcal{G}'}{\mathbf{E}}\Big[\sum_{\emptyset\neq S\subseteq \{1,...,2N\}}\hat{A}(S)\prod_{i\in S}P_iz_i\Big]\Big|\\
%&=\Big|\hat{A}(S)\prod_{i\in S}P_i\underset{z\sim \mathcal{G}'}{\mathbf{E}}\Big[\prod_{i\in S}z_i\Big]\Big|\\
%&=\Big|\hat{A}(S)\prod_{i\in S}P_i\hat{\mathcal{G}'}(S)\Big|\\
%&\leq \sum_{\emptyset\neq S\subseteq \{1,...,2N\}} |\hat{A}(S)|p^{|S|}\sqrt{e}^{|S|}|\hat{\mathcal{G}}(S)|\\
%&\leq \sum_{k=1}^{2N}(\sqrt{\varepsilon}p)^k\Big(\max_{S:\, |S|=k}|\mathcal{G}(S)|\Big)\sum_{S\subseteq\{1,\cdots,2N\}, |S|=k}|\hat{A}(S)|\\
%&\leq \sum_{k=1}^{2N}(\sqrt{\varepsilon}p)^k\Big(\max_{S:\, |S|=k}|\mathcal{G}(S)|\Big)(c\log s)^{k(d-1)}\\
%&=\sum_{K=1}^{2N}q^k\Big(\max_{S:\, |S|=k}|\mathcal{G}(S)|\Big)
%\end{split}
%\end{equation}
%
%Donde hemos definido $q:=\sqrt{\varepsilon}p(c\log s)^{d-1}$.  Si $k$ es impar entonces del lema \ref{} se sigue directamente que $\max_{S:\, |S|=k}|\mathcal{G}(S)|=0$. Si $k$ es par, i.e. $k=2l$ y además $l\leq \lfloor n/2\rfloor$ por el mismo lema tenemos la cota superior $\max_{S:\, |S|=k}|\mathcal{G}(S)|\leq l!\cdot N^{-l/2}$. Si por último k es par pero $l\geq \lceil n/2 \rceil=\lfloor n/2 \rfloor +1$ entonces $\max_{S:\, |S|=k}|\mathcal{G}(S)|\leq 1$, estas observaciones convierten la ecuación anterior a la forma
%
%\begin{equation}
%\Big|\underset{z\sim \mathcal{G}'}{\mathbf{E}}[A(P \circ z)]-A(0) \Big|\leq \sum_{l=1}^{\lfloor n/2 \rfloor}q^{2l}\cdot l!\cdot N^{-l/2}+\sum_{l=\lfloor n/2 \rfloor +1}^Nq^{2l}
%\end{equation}
%
%Como además, por hipótesis, $q$ es menor que $1/2$ entonces ...
%
%\begin{equation}
%\Big|\underset{z\sim \mathcal{G}'}{\mathbf{E}}[A(P \circ z)]-A(0) \Big|\leq 2q^2N^{-1/2}+2q^{n+1}\leq 3q^2N^{-1/2}
%\end{equation}
%\end{proof}
%
%\begin{lema}Sea $p\leq 1/4$ y sea $A:\{\pm 1\}^{2N}\rightarrow\{\pm 1\}$ un circuito booleano de tamaño $s$ y profundidad $d$ tal que
%\begin{equation}
%\sqrt{\epsilon}p(c\log s)^{d-1}\leq 1/4
%\end{equation}
%Si tomamos un $z_0$ en $[-1/2,1/2]^{2N}$ entonces se cumple que
%\begin{equation}
%\Big|\underset{z\sim \mathcal{G}'}{\mathbf{E}}[A(z_0+p\cdot z)]-A(z_0)\Big|\leq 12\varepsilon\cdot p^2(c\log s)^{2(d-1)}N^{-1/2}
%\end{equation}
%\end{lema}
%
%\begin{proof}
%Dado $z_0$ podemos definir una distribución de probabilidad $\mathcal{R}_{z_0}$ sobre el conjunto $\{-1, 1, *\}$ como sigue:\\
%
%Para cada $1\leq i\leq 2N$ tomamos $\rho_i=sgn((z_0)_i)$ con probabilidad $|(z_0)_i$ y $\rho_i=*$ con probabilidad complementaria $1-|(z_0)_i|$\\
%
%Si definimos $P\in [-2p,2p]^{2N}$ como $P_i=p\cdot \frac{1}{1-|(z_0)_i|}$ y tomamos $\rho\sim \mathcal{R}_{z_0}$ entonces para cada vector $z\in \mathbb{R}^{2N}$ podemos definir un vector $\tilde{z}=\tilde{z}(z,\rho)\in\mathbb{R}^{2N}$ como
%
%\begin{equation}
%\tilde{z}_i=\begin{cases}
%\rho_i\qquad &\text{si } \rho_i\in\{\pm 1\}\\
%P_iz_i\qquad &\text{en otro caso}
%\end{cases}
%\end{equation}
%
%Lo que probaremos a continuación es que par aun $z$ fijo, $\tilde{z}$ es una variable aleatoria que depende de $\rho$. De hecho veremos que la distribución de la v.a. $\tilde{z}$ es una distribución producto y que además su esperanza es exactamente $z_0+p\cdot z$. En particular se cumplirá que cada coordenada de $\tilde{z}$ es independiente de las demás y que la esperanza de cada coordenada es
%
%\begin{equation}
%\underset{\rho\sim\mathcal{R}_{z_0}}{\mathbf{E}}[(z_0)_i]=|(z_0)_i|sgn((z_0)_i)+(1-|(z_0)_i|)\cdot P_iz_i=(z_0)_i+pz_i
%\end{equation}
%
%En este punto, como $A$ es multilineal y $\tilde{z}$ tiene una distribución producto, usando la caracterización de la ecuación \ref{} para $A$ podemos escribir
%
%\begin{equation}
%\underset{\rho\sim\mathcal{R}_{z_0}}{\mathbf{E}}[A(\tilde{z})]=A(z_0+p\cdot z)
%\end{equation}
%
%Tomando un $z\sim\mathcal{G}'$ obtenemos
%
%\begin{equation}
%\begin{split}
%\Big|\underset{z\sim \mathcal{G}'}{\mathbf{E}}[A(z_0+p\cdot z)]-A(z_0)\Big|&=
%\Big|\underset{z\sim \mathcal{G}'}{\mathbf{E}}\underset{\rho\sim\mathcal{R}_{z_0}}{\mathbf{E}} [A(\tilde{z}(z,\rho))-A(\tilde{z}(0,\rho))\Big|\\
%&\leq \underset{\rho\sim\mathcal{R}_{z_0}}{\mathbf{E}} \Big[\Big|\underset{z\sim \mathcal{G}'}{\mathbf{E}}[A(\tilde{z}(z,\rho))]-A(\tilde{z}(0,\rho))\Big|\Big]
%\end{split}
%\end{equation}
%
%Pero para un $\rho$ fijo se tiene que $A(\tilde{z}(z,\rho))=A_\rho (P\circ z)$, donde $A_\rho$ se obtiene de $A$ fijando todas las coordenadas que están fijas en $\rho$. Por tanto
%
%\begin{equation}
%\begin{split}
%\Big|\underset{z\sim \mathcal{G}'}{\mathbf{E}}[A(z_0+p\cdot z)]-A(z_0)\Big|\leq \underset{\rho\sim\mathcal{R}_{z_0}}{\mathbf{E}} \Big[\Big|\underset{z\sim \mathcal{G}'}{\mathbf{E}}[A_\rho(P\circ z)]-A_\rho(0)\Big|\Big]
%\end{split}
%\end{equation}
%
%Y, por último, aplicando el lema \ref{} obtenemos que para cada $\rho$ fijo se cumple que
%
%\begin{equation}
%\Big|\underset{z\sim \mathcal{G}'}{\mathbf{E}}[A_\rho(P\circ z)]-A_\rho(0)\Big|\leq 3\varepsilon(2p^2)(c\log s)^{2(d-1)}N^{-1/2}
%\end{equation}
%
%Donde hemos usado que $P\in[-2p, 2p]^{2N}$ y que $\sqrt{\epsilon}p(c\log s)^{d-1}\leq 1/4$.
%\end{proof}
%
%\begin{prop} Sea $A:\{\pm 1\}^{2N}\rightarrow \{\pm 1\}$ un circuito booleno de tamaño $s$ y profundidad $d$, entonces
%
%\begin{equation}
%\Big|\underset{z\sim \mathcal{D}}{\mathbf{E}}[A(z')]-A(0)\Big|\leq 32\varepsilon(c \log s)^{2(d-1)}N^{-1/2}
%\end{equation}
%\end{prop}
%
%\begin{proof}
%Asumamos en primer lugar SPG que 
%
%\begin{equation}
%\sqrt{\varepsilon}\cdot(c\log s)^{d-1}\leq \frac{1}{4}N^{1/4}
%\end{equation}
%\end{proof}
%
%
%\begin{redBox}
%\begin{corol} Sea $A:\{\pm 1\}^{2N}\rightarrow \{\pm 1\}$ un circuito booleno de tamaño $\exp(\log^{O(1)}(N))$ y profundidad $O(1)$, entonces
%
%\begin{equation}
%|\underset{z'\sim \mathcal{D}}{\mathbf{E}}[A(z')]-\underset{u\sim U_{2N}}{\mathbf{E}}[A(u)]|\leq \frac{polylog(N)}{\sqrt{N}}
%\end{equation}
%\end{corol}
%\end{redBox}
%
%
%\begin{redBox}
%\begin{teo}
%Existe una distribución de probabilidad $\mathcal{D}$ sobre el conjunto de valores $\{1,-1\}^{2N}$ tal que:
%\begin{itemize}
%\item Existe un algoritmo cuántico con orden de ejecucuón $O(\log N)$ que distingue entre $\mathcal{D}$ y la distribución uniforme con ventaja $\Omega\Big(\frac{1}{\log N}\Big)$.
%\item Ningún dircuito booleano de tamaño $quasipoly(N)$ y profundidad constante distingue entre $\mathcal{D}$ y la distribución constante con una ventaja mayor que $polylog(N)/\sqrt{N}$.
%\end{itemize}
%\end{teo}
%\end{redBox}
%
%
%\begin{redBox}
%\begin{corol} En el modelo de la caja negra, $ Promise\BQP\nsubseteq Promise\PH$
%\end{corol}
%\end{redBox}
%
%\begin{redBox}
%\begin{corol} Existe un oráculo $O$ relativo al cual $\BQP^O\nsubseteq \PH^O$
%\end{corol}
%\end{redBox}
%
%\begin{redBox}
%\begin{corol} $Promise\,\mathbf{BQLOGTIME}\nsubseteq Promise\AC^0$
%\end{corol}
%\end{redBox}
%
%\begin{proof}
%
%\end{proof}

%\section{\texorpdfstring{$\BQP$}{BQP} en Hipercomputación Relativista}
%\index{Computación relativista}
%\index{Hipercomputación}
%
%\begin{redBox} \index{Espaciotiempo de Malament-Hogarth}
%\begin{defin}[Espaciotiempo de Malament-Hogarth] Un espaciotiempo $(\mathcal{M},g_{\alpha\beta})$ se dice de \textbf{Malament-Hogarth} (usualmente abreviado MH) si existe una semicurva de género tiempo $\gamma_1\subseteq \mathcal{M}$ y un punto $p\in \mathcal{M}$ tal que
%\begin{equation}
%\int_{\gamma_1}d\tau=\infty,\ \text{ y además }\ \gamma_1\subseteq \mathcal{I}^-(p)
%\end{equation}
%\end{defin}
%\end{redBox}
%
%\begin{redBox}  \index{CTC} \index{Cúrva de género tiempo cerrada}
%\begin{defin}[Curva cerrada de género tiempo] Una \textbf{CTC} es una curva de género tiempo en un espaciotiempo $(\mathcal{M},g_{\alpha\beta})$ que interseca no trivialmente consigo misma.
%\end{defin}
%\end{redBox}
%
%\begin{redBox}  \index{CTC} \index{Cúrva de género tiempo cerrada}
%\begin{prop} Si un espaciotiempo $(\mathcal{M},g_{\alpha\beta})$ contiene una CTC entonces es MH.
%\end{prop}
%\end{redBox}
%
%\begin{redBox}
%\begin{teo} Bajo la hipótesis de existencia de CTCs, $\PSPACE=\BQP=\PP$
%\end{teo}
%\end{redBox}
